docno="lists-007-15665869"
received="Sat Mar  3 11:57:06 2001"
isoreceived="20010303165706"
sent="Sat, 3 Mar 2001 08:56:24 -0800"
isosent="20010303165624"
name="Mark Nottingham"
email="mnot@akamai.com"
subject="Re: Recognizing search engine spiders"
id="20010303085622.B24919@akamai.com"
charset="us-ascii"
inreplyto="p05010402b6c6698120db&#64;[130.237.161.111]"
expires="-1"

To: Jacob Palme<jpalme@dsv.su.se>
Cc:discuss@apps.ietf.org



On Sat, Mar 03, 2001 at 10:31:02AM +0100, Jacob Palme wrote:
> Is there any standard which search engines use when sending
> HTTP requests during spidering, in order to tell the
> receipient HTTP server that they are search engines.

No. Some try to use a heuristic, with varying success (the big ones
are pretty easy to get).

> I can see multiple uses of this. In our particular case,
> we sometimes intentionally create slightly varying URLs
> of the same document, in order to stop an old version
> in the cache to be used. (Yes, I know there are cache
> control standards, but they do not seem to work in all
> cases.) This might mean that a search engine would
> store multiple copies of nearly the same document,
> and would not recognize that a new version replaces an
> old version of the same document.

There are ways to assure cache freshness. See
  http://www.mnot.net/cache_docs/
If that isn't good enough, vary the reference's query string, most
search engines will understand.

Also, you might try using robots.txt to shape which documents will be
fetched.


-- 
Mark Nottingham, Research Scientist
Akamai Technologies (San Mateo, CA USA)



