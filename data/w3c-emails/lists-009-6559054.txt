docno="lists-009-6559054"
received="Thu Aug 10 14:48:47 1995"
isoreceived="19950810184847"
sent="Thu, 10 Aug 95 17:45:54 0400"
isosent="19950810214554"
name="jg@w3.org"
email="jg@w3.org"
subject="Re: UDP or TCP?"
id="9508102145.AA29024@zorch.w3.org"
inreplyto="Pine.SOL.3.91.950810130851.454C100000&#64;chivalry"
expires="1"


To: Simon Spero<ses@tipper.oit.unc.edu>
Cc: Jeffrey Mogul<mogul@pa.dec.com>, Paul Leach<paulle@microsoft.com>,http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com

The major point, which has to be reiterated, is that protocols
the Web uses must implement congestion control.  While
what you say is all true (about images not needing in order delivery,
if you are willing to tolerate "interesting" display while arriving),
widespread deployment of software such as the WWW that does not
observe congestion control algorithms can cause the widespread collapse
of the Internet, as happened in the 1980's.  I, among others, still
have scars.  And the Web has not (yet) killed the Internet precisely
because it has observed congestion control (by being layered on TCP).

It is one thing if a uncontrolled protocol is .1% of the internet; it is
quite another for one which now represents such a large fraction of total
traffic.

You do raise an interesting point, true for some datatypes.
Audio data only needs to be real-time for teleconferencing; most
applications don't demand low latency operation.  But for some datatypes,
(such as audio) you might want to be able to change compression/quality on the
fly, if you find your link is not able to provide sufficient bandwidth
(which may vary on a relatively short timescale).  Provision for this
in NG may be a good thing; but the actual transport of the data better
observe congestion control.  This implies algorithms very much like those
that TCP already provides.
- Jim



