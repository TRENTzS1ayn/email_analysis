docno="lists-010-0068968"
received="Tue Mar 19 10:02:10 1996"
isoreceived="19960319150210"
sent="Tue, 19 Mar 1996 09:48:19 0800"
isosent="19960319174819"
name="Paul Hoffman"
email="paulh@imc.org"
subject="Re: About that Host: header...."
id="v03005503ad74a1fbe799@[165.227.113.247]"
charset="usascii"
inreplyto="9603191528.AA12825&#64;void.ncsa.uiuc.edu"
expires="1"


To:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com

>I think any response that is hidden from the users (ie handled
>automatically by
>a client with a retry) will simply increase bandwidth without any noticable
>noise from users.  Without noise from the users, I doubt alot of sites will
>upgrade.  I oppose the retry idea.

I strongly agree with Beth here. Silently retries on 1.0 servers is going
to increase bandwidth, decrease functionality to the users, and probably
not be cared about by HTTP admins. This assures us that when we go to 1.2
(full URI in request), there will be mass breakage on the Internet, and all
fingers will point at the IETF. Again.

Jim's (4) is best in my mind. With this scheme, someone can easily write a
testing robot that wanders the Web checking for conformant and
non-conformant hosts, creating a list of shame for those that run buggy
server software. Trust me, sites that get on this list will either demand a
quick upgrade of their software or switch vendors. Wearing my WebCompare
hat, I'll certainly write and run such software, but I imagine others in
the press will beat me to the punch, and that's just fine with me.

--Paul Hoffman
--Internet Mail Consortium



