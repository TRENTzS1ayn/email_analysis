docno="lists-010-10914254"
received="Sat Oct 19 13:46:56 1996"
isoreceived="19961019174656"
sent="Sat, 19 Oct 1996 16:45:51 0400"
isosent="19961019204551"
name="Keith Moore"
email="moore@cs.utk.edu"
subject="Re: PEP Battle Plan [rexmit, garbled]"
id="199610192045.QAA21687@ig.cs.utk.edu"
inreplyto="PEP Battle Plan [rexmit, garbled]"
expires="1"


To: Rohit Khare<khare@w3.org>
Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com,moore@cs.utk.edu

> Progress on an extension mechanism is essential because it is the future of
> 1.x and binary encodings of it.

I realize this is heresy here, but I have to wonder if it's worth
building the extension mechanism into HTTP.  An efficient URI
resolution protocol would allow for a smooth transition away from
HTTP 1.x and to 2.x or other protocols (smb? webnfs? multicast?), 
without invalidating old clients and without the overhead of 
establishing a TCP connection.  New protocols could then be designed 
from scratch to take into account everything that has been learned 
from HTTP, without inheriting the complexity.

It would also improve scalability, fault tolerance, ability to 
screen files (for content ratings, price, language, etc.) before 
downloading, selection of multiple variants of a resource (by 
allowing the client, rather than the server, to make the selection),  
client selection of multiple locations of a resource, etc. 

Seems like we need to take a step back and look at the web as a
whole before we commit to the direction of adding more complexity
to HTTP.

Keith



