docno="lists-010-4641898"
received="Tue Jul  9 21:19:57 1996"
isoreceived="19960710011957"
sent="Tue, 09 Jul 1996 21:06:54 0700"
isosent="19960710040654"
name="Roy T. Fielding"
email="fielding@liege.ICS.UCI.EDU"
subject="Re: multihost virtual sites for HTTP 1.2"
id="9607092106.aa26402@paris.ics.uci.edu"
inreplyto="c=US%a=_%p=msft%l=RED-77-MSG-960709215536Z17122&#64;abash1.microsoft.com"
expires="1"

To: Paul Leach<paulle@microsoft.com>
Cc:http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com


> What I'd like to see in HTTP/1.2 is the ability to easily and
> efficiently make multiple hosts act like a single virtual Web site, so
> that one can build huge web sites.

Ummm, assuming it is decent server software, the only real limitation
on site size right now is the bandwidth of the incoming network.
Splitting requests onto different server machines can be easily
accomplished using a very fast gateway box which sits on the trunk
and routes requests to the internal servers for processing.  The only
thing the gateway would need to do is look for the beginning of each
message and multiplex from there (which is more difficult than
IP routing, but not much more).

I think that would be a lot easier than changing HTTP to accomodate
complex URL rewriting rules.  There are good reasons to support a
URI rewriting mechanism on the client side (e.g., to make the URI
syntax truly extensible, support automated mirroring, support URNs,
etc.), but I haven't considered single-site performance to be one
of them.

 ...Roy T. Fielding
    Department of Information & Computer Science    (fielding@ics.uci.edu)
    University of California, Irvine, CA 92697-3425    fax:+1(714)824-4056
    http://www.ics.uci.edu/~fielding/



