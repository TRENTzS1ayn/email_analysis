docno="lists-011-4608275"
received="Fri Jul 18 17:21:51 1997"
isoreceived="19970718212151"
sent="Fri, 18 Jul 1997 20:14:07 0400"
isosent="19970719001407"
name="Joel N. Weber II"
email="devnull@gnu.ai.mit.edu"
subject="Re: STATUS100 Re: Proposed resolution"
id="199707190014.UAA12266@mescaline.gnu.ai.mit.edu"
inreplyto="Pine.BSF.3.96.970717153147.17807C100000&#64;shell3.ba.best.com"
expires="1"


To:gjw@wnetc.com
Cc:rlgray@raleigh.ibm.com,http-wg%cuckoo.hpl.hp.com@hplb.hpl.hp.com

   Date: Thu, 17 Jul 1997 15:47:13 -0700 (PDT)
   From: "Gregory J. Woodhouse" <gjw@wnetc.com>
   X-Url: http://www.wnetc.com/

   To me, it seems like the real problem is that the server has no way of
   knowing how much data to expect. Accepting a chunked PUT or POST is an all
   or nothing type of commitment. I doubt it's possible in HTTP/1.1, but it
   seems to me that the server need to be able to indicate how much data it
   is willing to accept and then allow the client to decide whether or not to
   attempt to send the request. (A client may not know how much data it has
   to send, but it may know that it will not exceed a certain threshold.)

In general, I've been taught to write programs that don't have arbitrary
limits, so I think I would hate to write a server which places a limit
on the size of a POST request.

Perhaps for a search engine, it might make sense to create some restriction
on the size of such data; but I think no matter what you do you're
open to denial-of-service attacks to some extent.



