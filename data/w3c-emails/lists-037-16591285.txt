docno="lists-037-16591285"
received="Sun Sep 21 18:43:59 1997"
isoreceived="19970921224359"
sent="Sun, 21 Sep 1997 15:39:29 -0700"
isosent="19970921223929"
name="Arthur van Hoff"
email="avh@marimba.com"
subject="Re: Collections"
id="3425A221.FA101E23@marimba.com"
charset="us-ascii"
inreplyto="01BCC2D0.A7A18DA0.ejw&#64;ics.uci.edu"
expires="-1"


To:"ejw@ics.uci.edu"<ejw@ics.uci.edu>
CC:"'Judith Slein'"<slein@wrc.xerox.com>,"'w3c-dist-auth@w3.org'"<w3c-dist-auth@w3.org>

Hi Jim,

Jim Whitehead wrote:
> My fear with returning a full depth infinity index for all cases is that
> some cases (e.g., collections high up in a hierarchy) may return very large
> results, which could cause a problem for low-memory clients.

This is a valid concern if you assume that indexes can be generated for
arbitrary portions of any web-site. This was not the intention in the
DRP
proposal. The assumption is made that a server will provide indexes only
for those portions of the site that make sense to be replicated. For
example
in our server you need to specify for which parts of your site the
server
will generate indexes.

By the way, sometimes it makes absolute sense to generate exhaustive
indexes for entire sites. You may allow this for web site replication
for example. In that case you could either generate the index
dynamically,
or you can generate it offline and store it in an index file in a
portion of the site to which access is restricted. Again, these indexes
would define complete self-consistent snapshots of the site as it was
at a particular moment in time.

Have fun,

Arthur van Hoff




application/x-pkcs7-signature attachment: S/MIME Cryptographic Signature




