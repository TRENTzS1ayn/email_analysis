docno="lists-041-4103207"
received="Wed Jan  9 00:31:49 2002"
isoreceived="20020109053149"
sent="Tue, 8 Jan 2002 21:29:55 -0800"
isosent="20020109052955"
name="Lisa Dusseault"
email="lisa@xythos.com"
subject="RE: Interest in standardizing Batch methods?"
id="HPELJFCBPHIPBEJDHKGKCENLDDAA.lisa@xythos.com"
charset="us-ascii"
inreplyto="20020107183316.J6499&#64;lyra.org"
expires="-1"

To:"Greg Stein"<gstein@lyra.org>,"Jim Whitehead"<ejw@cse.ucsc.edu>
Cc:"WebDAV"<w3c-dist-auth@w3.org>



> Personally, I'm going to guess they didn't pipeline requests, so a batch
> mechanism was a must to get around deficiencies in their protocol stack.

There's potentially a little more to it than that.
(1) Imagine a client selects a bunch of resources and drags to move them all
to a different folder.  A batch MOVE operation can do those in one
transaction, so that the whole request fails if not all can be moved.  This
becomes rather more important if the client is actually using an API
(MSDAIPP??) that offers large-scope operations, yet how can it guarantee
that operation will work or won't work if it can only send it piecemeal to
the server?

(2) See Yaron's email
(http://lists.w3.org/Archives/Public/w3c-dist-auth/1998OctDec/0303.html)
about why pipelining doesn't always work (can't always be used even when
available).  I don't know to what extent pipelining is realistically
unavailable/unworkable.

That said, it's still not clear batch methods are so necessary they'd
preempt other work we've got to do.

Lisa



