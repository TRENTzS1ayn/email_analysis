docno="lists-055-3750154"
received="Fri Apr 18 05:52:26 1997"
isoreceived="19970418095226"
sent="Fri, 18 Apr 97 10:52:12 BST"
isosent="19970418095212"
name="Henry S. Thompson"
email="ht@cogsci.ed.ac.uk"
subject="Re: A serious detail point"
id="1338.199704180952@grogan.cogsci.ed.ac.uk"
inreplyto="Thu, 17 Apr 1997 12:10:26 -0700"
expires="-1"


To:w3c-sgml-wg@w3.org

Tim writes:
>  So the overhead is only (name-of-entity + 2) characters.  This looks
>  like a good solution to me.  Henry says it's
>  
>  a) obnoxious [ which I'm sure he will agree requires more explication
>     to be a useful argument ], and

Of course you're right -- shorthand for irritatingly repetitive.

>  b) inefficient in the absence of caching.  True, but that would also
>     be true if you were to re-use URLs as HT proposes... I really don't
>     see an efficiency win either way.
>  
>  There's a big problem with remembering the last URL for re-use; the
>  problem of maintaining state.  If I have a collection of 5,000 such
>  URLs, and I need to insert one pointing at something else, then, I'm
>  going to have to remember to re-establish context after that pointer.
>  Second, if I use an XML-link from *outside* into that list of 5,000
>  pointers, if we use the #CURRENT-like method, I have to read them
>  all in series to make sure the one I'm pointing at is interpreted
>  correctly.  If I use the &c;#... method, then I only have to read
>  the internal subset before jumping to ID(p324). -T.

I think Peter's criticism was similar, if I understood it.  I accept
this as a knock-down rebuttal.  Sigh.

ht



