docno="lists-056-0283930"
received="Sat Jun 14 21:13:15 1997"
isoreceived="19970615011315"
sent="Sat, 14 Jun 1997 21:13:00 -0400"
isosent="19970615011300"
name="Dave Peterson"
email="davep@acm.org"
subject="Re: I18N issue needs consideration"
id="v01540b03afc8ebc6890a@[207.60.235.15]"
charset="us-ascii"
inreplyto="I18N issue needs consideration"
expires="-1"


To:w3c-sgml-wg@w3.org

At 9:33 AM 6/13/97, Tim Bray wrote:

>         The position I'm advancing is that XML do the same
>deliberate abandonment of abstraction at the character level,
>saying characters are indeed the bit patterns described in
>Unicode, with the semantics and processing characteristics
>described in Unicode, and that's all there is to it.

It appears you don't understand what abstraction is.  The
characters are still abstract; you're just asking that all
XML systems be required to use the same representation, at
least at one point in their internal processing (i.e., the
interface between their "XML processor" routine and their
"application" routine).

Furthermore, the representation you suggest makes sense
only for character strings, not individual characters,
since it represents some characters with a 16-bit bit
combination and others with a 32-bit bit combination.
like saying I'll represent small integers with one octet
but larger integers with two, with no indication at compile
time as to which to use.  Duh.

Specifying the internal representation for characters makes
about as much sense as saying that the system *must* use
twos-complement representation for negative integers.
I've worked on systems that didn't even use the canonical
representation for non-negative integers.  Will they also
be deemed unacceptable?

Leave it up to the system designers.

Dave Peterson
SGMLWorks!

davep@acm.org



