docno="lists-059-12887108"
received="Wed Feb 28 16:32:56 2001"
isoreceived="20010228213256"
sent="Wed, 28 Feb 2001 16:35:08 -0500"
isosent="20010228213508"
name="Jutta Treviranus"
email="jutta.treviranus@utoronto.ca"
subject="Agenda revision"
id="p04320402b6c31b34d118@[64.134.48.76]"
charset="us-ascii"
expires="-1"


To:w3c-wai-au@w3.org

Len and I have talked further about the Thursday morning agenda. We 
felt that it would be valuable to talk about the evaluation of 
authoring tools on Thursday.

In reviewing the issue I came up with the following possible 
evaluation processes:

1) A manual checkpoint by checkpoint evaluation by an informed 
evaluator who knows ATAG and is reasonably familiar with the tool.
2) Testing of the tool by creating a representative test document 
with the tool and assessing the outcome using WCAG and assessing the 
tool using a guideline 7 checklist for the specific task.
3) Either of the above using a checklist or test document specific to 
the class of tool.
4) Recruiting 8 average users of the tool who know little if anything 
about access and asking them to create a test document using the tool 
and then assessing the outcome using WCAG. In addition recruiting 
authors with disabilities and asking them to create a test document 
and assessing the accessibility of the task using the tool.

Half of these will give comparative data if done consistently but 
can't be used to make a definitive conformance statement. The other 
half takes a long time but will arrive at a definitive conformance 
statement.

What are peoples thoughts?

Jutta



