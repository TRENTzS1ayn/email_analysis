docno="lists-066-15102329"
received="Mon Nov 22 22:30:26 1999"
isoreceived="19991123033026"
sent="Mon, 22 Nov 1999 19:30:23 -0800 (PST)"
isosent="19991123033023"
name="Scott Luebking"
email="phoenixl@netcom.com"
subject="Re: A brief analysis of dynamically generated web pages and"
id="199911230330.TAA00444@netcom.com"
inreplyto="A brief analysis of dynamically generated web pages and"
expires="-1"


To:charles@w3.org,phoenixl@netcom.com
Cc:unagi69@concentric.net,w3c-wai-gl@w3.org

Hi, Charles

You may have misread Greory's posting.  In my note, I was addressing the section
of his comments where he says:

  "yes, the server should assume half of the burden (which can be done via
  responsible browser sniffing, so that the stylesheet and inline style
  declarations delivered with the content"

I disagree that client side style sheets can create the best document
for a person's needs.  In my note to Gregory, I brought up the issue of
the "concept-barrier" in computer science.  This is key to understanding
the limitations of both access technology and client side style sheets
because they both share the same problem.


Suppose you have a collection of blocks of text with medical information.
You need to hire someone to sort through the blocks of text and create a
page where the information is presented in a way that will make it
easy for the reader to understand.  Two people apply.  One person understands
medicine and its various concepts.  The other has no medical background.
Who would more likely structure the web page in a way where the blocks
of text are organized in an order which is logical for the concepts
and also in an order of importance also indicated by the concepts.

A server has a much greater chance of organizing a web page to match
the concepts.  For example, the user requests information in order
of a particular scientific ranking and then by geographic area.  The server
can do that.  However, client side stylesheets and access technology
do not understand the concepts being used.  Further, they are limited
in their abilities to deduce the concepts, e.g. understanding the
ranking, since they are working with very limited information,
i.e. the web page, which has probably had the key ranking information obscured
in the text.

An important factor in optimisation is that the more information available,
the better to optimisation.  The server generally has more information.

You might argue that the client-side technology might do better, but how will
that work?  For example, how would client side technology determine what
are more important links and less important links so it knows the order?
Given two forms on a page, how will the client side technology understand
which form does what?  Otherwise, the user will have to search.

Take a look at the demo I set up.  Check out the standard form.
Then check out the blind version.  How would client side technology
be able to do the same reorganization?  (no cheating  -  can't just wave
your hands and hand it off to the client side programmers.  <smile>)

Scott


> In Gregory's model, as I understand it, the user (client) has access to
> specialised style sheets, which suit their individual needs. They can apply
> that to well-structured content to get  document htat fits their personal
> requirements much more precisely than a server-sided approach can approximate
> the requirements of a class of users. In addition, the client can determine
> whether they want images, or audio content, or other separate pieces at the
> time they read. They can use bookmarks that are the same whether they are
> sighted, deaf, blind, etc.
> 
> In some cases there is value in serving content of different types according
> to diffferent requests. (An obvious one is the tablin service that linearises
> tables and adds headers according to specified options). But in many cases
> the best solution is still to serve well-created content the first time
> around.
> 
> Charles



