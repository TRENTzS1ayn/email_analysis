docno="lists-068-5190792"
received="Fri Dec 29 20:50:36 2000"
isoreceived="20001230015036"
sent="Fri, 29 Dec 2000 20:54:33 -0500"
isosent="20001230015433"
name="Al Gilman"
email="asgilman@iamdigex.net"
subject="Re: Question on abbreviations (fwd)"
id="200012300150.UAA249670@smtp2.mail.iamworld.net"
charset="us-ascii"
inreplyto="030201c071da$af896440$6401a8c0&#64;sttln1.wa.home.com"
expires="-1"


To:"Matt May"<mcmay@bestkungfu.com>,"Leonard R. Kasday"<kasday@acm.org>
Cc:"WAI GL"<w3c-wai-gl@w3.org>

At 01:02 PM 2000-12-29 -0800, Matt May wrote:

[note an earlier message where I said "we need to do both."]

>
>I don't understand the insistence on tagging every individual instance.
>(Note: I do understand why they need to be expanded, just not why they need
>to be tagged.) User agents will need to be revised to handle new markup
>anyway (what supports acronym and abbr at present?). If this technology is
>in the user agent, then _every site_ would be incrementally more accessible.
>The opportunity is there, not here.
> 

Stuff independent of the author works where the intended sense is drawn from
common usage that would be documented in a third-party dictionary, and where
isolating the intended sense among the documented senses is not too hard.

In cases where the intended sense is not common or is too close for comfort to
another sense that could fit in the same context, it is essential to have
information from the author.  In the latter case markup [loosely construed,
whether in the document or in RDF resources on the site] is the way to go.

My ideal process includes lots of breakpoints.  It is easy in an authoring
environment to trap tokens where a) there is no widely known meaning or b)
there is widespread confusion about what it means.  These are automatically
generatable query-warnings.  These could be parametric in reading level, too,
if one wished.  International vs. national vocabulary is another option a lot
like reading level.  Then there are other conditions that are not based in
generic global dictionaries and rule bases, but on knowledge of the current
context.  If a token matches a glossary entry in the current document or
anywhere in the current project or any project in the same line
organization as
the current project, that might be sufficient reason to raise a query as to
whether this term needs marking or not.  Not just line organizations; get the
author to declare at the outset what knowledge domains they are touching in
this volume.  Then the vocabulary assistant can be loaded with knowledge bases
that go with these domains; and terms which are known to require some emphasis
or explanation in the writings of those domains can be trapped and reviewed
for
marking.

It seems about time to start asking what we are going to _do_ about this.

Matt, to convince people to back off on demands against the content providers,
we need a working demonstration of what can be done in the User Agent
independent of the author.  Actually I want several people including people
with disabilities to try out GuruNet.  Then we can see if we want to create
something new or just use that as our proof of concept.

Even 'though I said User Agent, if we need to put anything new together it
sounds like that belongs in the Evaluation and Repair group for the time
being.  If we can demonstrate the effectiveness of a repair strategy, it could
be possible to negotiate more modest demands to be made of the content
providers (back here).

Then there are the author side techniques.  What we really want people to do
[especially with the WAI documents] is not that they should document the funny
meanings that go with their tortured word use, but that they should reflow
their statements into something that only depends on common terminology, and
still gets the same point clear.  Define neologisms or use arcane terms only
when this process breaks down.  Here we need methods to detect terms that are
prone to misunderstanding, and rewrite aids such as a thesaurus, and fuzzy
searching over existing dictionaries and glossaries, to find precedents in
prior art.

Al 



