docno="lists-069-16390620"
received="Fri Jan 18 02:41:14 2002"
isoreceived="20020118074114"
sent="Fri, 18 Jan 2002 18:41:05 +1100"
isosent="20020118074105"
name="Jason White"
email="jasonw@ariel.ucs.unimelb.edu.au"
subject="&quot;Until User Agents&quot; in WCAG techniques for 2.0"
id="15431.53649.978710.252872@jpc.local"
charset="us-ascii"
expires="-1"


To: Web Content Guidelines<w3c-wai-gl@w3.org>

So far we have been very successful in expunging "until user agents"
qualifications from the WCAG 2.0 guidelines themselves. However, the
question of user agent capabilities reappears at the technique level.
Given that the techniques are expected to be updated more frequently
than the guidelines, this enables, in principle, more consistent
maintenance of details that depend on the status of implementations.
However, there still remain important issues to be considered. As may
be anticipated, they also raise serious questions regarding the
relationship between our guidelines and policy formation, whether at
an organizational or governmental level. I shall summarize the
arguments which have been advanced on this topic.

1. It is indisputable that content developers would benefit from
   details as to the implementation of specific technologies,
   including the precise aspects on which particular techniques
   offered by this working group depend. The task of providing such
   information is complicated by several considerations:

a. The interaction of assistive technologies and user agents, which
multiplies the number of cases that need to be examined.

b. Variation in the availability and frequency of update, of
internationalized or localized versions of software (a point noted by
Charles).

c. The frequency with which new programs, and revised versions of
existing programs, are released.

d. The research required to collect reliable data identifying what
software (including assistive technologies) is actually being used,
which versions and by what proportion of users. Obviously, these
details will vary depending on what user population is considered for
purposes of the analysis. Variations may be introduced by a number of
factors including internationalization issues, economic circumstances,
(lack of) awareness and organizational policies.

2. There are, furthermore, underlying questions that need to be
   decided:

a. If statistical assessments of actual technology deployment are
relevant, where should the boundary between support and lack of
support for any given feature be drawn? To be specific, is there a
certain proportion of users, and if so what proportion, that need to
have implementations of a given feature before it can be deemed
supported for purposes of guiding developers?

b. How much flexibility should content developers have in defining the
characteristics of their intended audience? For example, if the web
site is likely or intended to be used by people who are expected to
have access to a particular technology, including, where necessary,
relevant assistive hardware and software, is it reasonable to design
the content on the assumption that the necessary support is available?
What considerations can a content developer legitimately take into
account in making such a determination and what kind of information
can we provide that would best asisst that decision process, given the
complexities mentioned above?

c. What is the relationship between any details that we might provide
regarding user agent support, and issues of policy? On the one hand,
any data that we were able to obtain might be of value in the process
of policy development. On the other hand, if for example a government
were to set a policy which involved a different level of technology
support than that indicated by this working group, developers within
the relevant legal jurisdiction would need to follow their
government's regulations rather than any advice we might offer. In
that case, our advice could become misleading, though this could be
remedied by appropriate disclaimers.

Also, in a complaint-based system of anti-discrimination law, how
relevant would general standards of technology support be in
determining whether the legal requirements had been met? For example,
the question is often not whether widely accepted standards of
accessibility have been followed, but whether the respondent has done
everything that they reasonably can (without incurring unjustifiable
hardship/burden etc.) to place persons with a disability on an equal
footing to persons without a disability in accessing the content.
Guidelines such as ours can serve as excellent evidence of measures
which may reasonably and appropriately be taken to provide access, but
any assumptions that this working group might endorse regarding levels
of technology support, may not be relevant, especially if the
developer could have avoided making the assumption in the design of
the content without incurring unjustifiable hardship. Thus the legal
standard may be very different from any more or less arbitrary
borderline that we might choose to draw, and again, affected
developers would need to follow the legal standards within the
jurisdiction instead of our guidance in such cases.

3. Having decided what kind of information we can most usefully
   provide to developers and how it should be collected, the final
   question is one of resources: who is going to obtain the needed
   information, keep it current and conduct any required research?
   Within the limitations of the resources available to this group,
   what kind of information can we most usefully provide and to what
   extent will it benefit developers?

If we answer these questions, a strategy for dealing with the issue of
user agent capabilities should emerge.



