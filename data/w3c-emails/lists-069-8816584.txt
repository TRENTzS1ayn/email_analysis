docno="lists-069-8816584"
received="Mon Dec  3 02:55:49 2001"
isoreceived="20011203075549"
sent="Mon, 3 Dec 2001 01:54:04 -0600"
isosent="20011203075404"
name="Gregg Vanderheiden"
email="GV@TRACE.WISC.EDU"
subject="request and summary"
id="001201c17bcf$b4a5dc60$3014d3d4@750"
charset="iso-8859-1"
inreplyto="a05101001b82d650e11a5&#64;[10.0.1.33]"
expires="-1"

To:"GLWAI Guidelines WG \(GL - WAI Guidelines WG\)"<w3c-wai-gl@w3.org>


Please people,

Anything can be made to sound ridiculous if you try.  Especially if you
change the words in the process.   When responding to ideas, try to
avoid rephrasing them then critique your own phrasing.   

Also, If things don?t make sense,  then a first assumption should be
that you don?t understand something.  And you ask polite questions.

Perhaps you misread something.  Perhaps the writer mistyped something.
Perhaps it just wasn?t explained well or completely.  Perhaps your
question will spark something that the writer hadn't thought of.

All of these are better explored with questions than indignation.

This is true for individual postings -- but especially true for postings
out of our working groups.


Now, here is a summary of what was done.


1)  a consensus was reached some time ago that we should figure out
which items were objective and which were subjective.  The goal is to
make as many items objective as possible since it is a problem to have a
'checkpoint' where people can't reliably decide or determine if it has
been met.

2) we also said some time ago that we would have to test our guidelines
this time before we released them.   

3) those people on the weekly conf call consensed on a test for whether
something was objective and we posted the test to the list for comment.
Unfortunately (my fault) I phrased it as a definition of objective
rather than a test.   I apologize.

4)  Then we began to examine our guidelines to see if we thought they
would pass or fail the test when it was done someday.  

This is what any developer does when designing.  
 -  They figure out what they are trying to achieve.
 -  They develop a test criterion for determining if they have achieved
their design goal.
 -  Throughout the process of design they look at their work and see if
they think it will fail.   If they judge that it will, they work on that
aspect.   These guesses are not in lieu of the real testing.  They are
just to get the item ready for testing and to remove any obvious errors
that they can pick out by just looking at it.


As we get our guidelines in better shape, -- and remove all the problems
that we can see ourselves by just looking at them, -- we will move into
testing them.   
They need to be tested for a number of different things including (but
not limited to)
- objectivity
- do-ability
- understandability
- usability 
etc.   
We will need to carefully design each type of test and figure out who
the test subjects should be etc.   as well as seeing if the different
tests can/should be carried out together or separately. 


I will be posting another longer document on our overall procedures
later.


Greetings from Brussels.

Gregg

-- ------------------------------ 
Gregg C Vanderheiden Ph.D. 
Professor - Human Factors 
Dept of Ind. Engr. - U of Wis. 
Director - Trace R & D Center 
Gv@trace.wisc.edu <mailto:Gv@trace.wisc.edu>, <http://trace.wisc.edu/> 
FAX 608/262-8848? 
For a list of our listserves send ?lists? to listproc@trace.wisc.edu
<mailto:listproc@trace.wisc.edu> 


-----Original Message-----
From: Kynn Bartlett [mailto:kynn-edapta@idyllmtn.com] 
Sent: Friday, November 30, 2001 10:38 AM
To: GV@trace.wisc.edu; GLWAI Guidelines WG (GL - WAI Guidelines WG)
Subject: Re: last part of todays telecon

At 5:37 PM -0600 11/29/01, Gregg Vanderheiden wrote:
>We began by reviewing the guidelines one at a time to determine whether
>or not:
>1.they met the ?80% or better? (80%+) objectivity criterion
>
>For number 1, "Provide a text equivalent for all non-text content?, we
>found:
>?We believed it would pass the  ?80%+? objective test
>
>For guideline number 2, "Provides synchronized media equivalence for
>time dependent presentations, we found:
>?We believed items 1 and 2 would pass the 80%+ objectivity test

Now I'm getting even more more weirded out by this "objectivity"
we've embraced.  In my last email, I said:

      So 80% of people, who meet subjective criteria for inclusion,
      then make subjective determinations, and if they happen to agree,
      we label this "objective"?

Apparently the way we are using our newfound "objectivity" criteria
is as follows:

      A group of people -- who may or may not meet subjective criteria
      for inclusion -- "reach consensus" on whether or not they
      subjectively believe that at least 80% of an undefined group of
      people -- who meet subjective criteria for inclusion -- would
      make agreeing subjective determinations on arbitrary undefined
      specific applications, ... and we label this "objective?"

This is newspeak of the worst kind, folks.  If we want credibility
for our work, we don't suddenly label as "objective" things which are
clearly and absolutely subjective.  A subjective decision doesn't
suddenly become objective if you vote on it.

The specific process you've defined may or may not be useful and I'm
not suggesting we reject that out of hang -- but if you keep it, you
MUST rename it to something else OTHER than "objective."

--Kynn

-- 
Kynn Bartlett <kynn@idyllmtn.com>
http://www.kynn.com/



