docno="lists-071-9141904"
received="Thu Sep 11 10:31:25 2003"
isoreceived="20030911143125"
sent="Thu, 11 Sep 2003 10:31:25 -0400 (EDT)"
isosent="20030911143125"
name="Charles McCathieNevile"
email="charles@w3.org"
subject="RE: Screen reader invisibility"
id="Pine.LNX.4.55.0309111027120.15993@homer.w3.org"
charset="US-ASCII"
inreplyto="B3DC65CD2AA7EF449E554548C6FE1111E0A92B&#64;MAIL01.austin.utexas.edu"
expires="-1"

To: John M Slatin<john_slatin@austin.utexas.edu>
Cc:tcroucher@netalleynetworks.com,rscano@iwa-italy.org,w3c-wai-gl@w3.org



There is a partial implementation of ACSS in emacspeak, and the server-side
product speakthis, from fonix.com also uses ACSS (it generates the whole page
in an mp3 and can do straightforward styling properties for pitch, volume,
etc).

It should be possible to get the style properties from the CSS DOM in a
modern browser, and then apply them as a speech-based user agent by hooking
them onto whatever they use to select styling in their implementation.

cheers

Chaals

On Thu, 11 Sep 2003, John M Slatin wrote:

>
>Some screen readers *do* actually read the source code and walk the DOM
>rather than simply pulling out what's on the screen.  JAWS is an
>example.
>
>One way to address this issue would be to provide support for the Aural
>CSS specification which is part of CSS 2.  My research group here at UT
>Austin is supporting a proof-of-concept project which has been going on
>since last spring in collaboration with some classes in the Computer
>Sciences department here.
>
>pwWebSpeak, IBM Home Page Reader, and now JAWS 5.0 (public beta) are
>doing what you might call "user-side ACSS"-- that is, these tools allow
>the end user to associate various sounds (.wav files, etc.) with
>specific elements such as headers.
>



