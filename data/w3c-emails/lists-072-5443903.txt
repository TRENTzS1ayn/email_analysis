docno="lists-072-5443903"
received="Thu Feb  5 10:30:46 2004"
isoreceived="20040205153046"
sent="Thu, 05 Feb 2004 16:59:44 +0200"
isosent="20040205145944"
name="lisa seeman"
email="seeman@netvision.net.il"
subject="RE: simple language testable thing"
id="014b01c3ebf8$b3db8230$340aa8c0@patirsrv.patir.com"
charset="us-ascii"
inreplyto="Pine.LNX.4.55.0402041340410.14689&#64;homer.w3.org"
expires="-1"

To: 'Charles McCathieNevile'<charles@w3.org>, 'Mike Barta'<mikba@microsoft.com>
Cc: 'Jens Meiert'<jens.meiert@erde3.com>,y.p.hoitink@heritas.nl,w3c-wai-gl@w3.org



In my proposal I was trying to  separate two things.

Point 1 / case 1. Requiring knowledge of a second language is an
enormous barrier of entry. However it may be hard to absolutely test
what is a foreign word.

I am not too sure about this, because there is usably a definitive
source of a language - and a simple parser should do it. But I can see
that this can be a trick one.

Point 2 / case 2. This barrier to entry is even higher for the user,
when the user needs to know a different alphabet to understand content.
For example and English word in a Chinese site. Hear you can not even
sound out the word, and see if you recognize it. 

What is more, in this case the testing is simple. - As simple as a one
line pattern match, - if you are outside the known range of letters and
characters, you are in foreign territory.

That is why I suggested that at least case 2 be considered as a P1
thing. We can then argue about case 1 and what to do with it...


A typical use case:
 In some countries people  assume that you have English as a second
language. However a dyslexic child will probably have to concentrate on
getting one language and will not learn a second language. This will
make a lot of sites inaccessible in part...

All the best
Lisa Seeman
 
Visit us at the UB Access website
UB Access - Moving internet accessibility
 


> -----Original Message-----
> From: Charles McCathieNevile [mailto:charles@w3.org] 
> Sent: Wednesday, February 04, 2004 8:54 PM
> To: Mike Barta
> Cc: lisa seeman; Jens Meiert; y.p.hoitink@heritas.nl; 
> w3c-wai-gl@w3.org
> Subject: RE: simple language testable thing
> 
> 
> On Wed, 4 Feb 2004, Mike Barta wrote:
> 
> >
> >so I would need to translate serbian within a hrvatskii page but not 
> >latin in an english page?
> 
> No, I think Lisa's original point was that this is easy for 
> hebrew (since the only other language I know of written in 
> the hebrew alphabet is yiddish. On the other hand, maybe 
> that's more common a use case than we suspect - I don't know 
> since I can't read either of them).
> 
> >personally I can see cases where use of a foreign bon mot, 
> even though 
> >readers may not know the meaning, or a foreign acronym, e.g. 
> CERN, is 
> >appropriate without translation.  in such cases the author 
> must decide 
> >what they want to do and whether the use is appropriate for their 
> >audience.
> 
> I think this issue is related to the question of what is 
> clear language. I think there is a fair argument that "bon 
> mot" is an english phrase in the rich english of literature 
> (or the literary english of the rich, perhaps).
> 
> But it isn't simple vocabulary one can expect of everyone. I 
> think the solution technique is the same as for complex 
> vocabulary - being able to do a glossary lookup, or even run 
> the document through a proxy that does one automatically, 
> perhaps giving a result like this (but not this one - this 
> isn't baked enough):
> 
> ... use of a foreign <a 
> href="http://example.com/k-7glossary?bon_mot">bon
> mot</a>, even though...
> 
> or even adding a helpful style sheet and giving the following
> 
> ... use of a foreign <ruby class="coolGloss"><rb>bon 
> mot</rb><rt>clever word or two</rt></ruby>, even though...
> 
> This sort of thing is done automatically by systems such as 
> the idea of smart links that was floated by Microsoft a while 
> ago, or WikiWords which are automatically identified by Wiki 
> systems. The Microsoft system I think ran from a glossary, 
> whereas WikiWords are triggered by a (slightly) more powerful 
> pattern match. I believe that industrial text translation 
> support software does this sort of thing routinely, but I 
> haven't tried it.
> 
> > I agree with the intent of your suggestion but the impact 
> of it could 
> >be to ban nearly all english literature from the web due to the many 
> >uses of foreign phrases and obscure words.  this issue is 
> fraught with 
> >subjective calls.
> 
> Well, not ban. Just state (if we adopt the proposal) that 
> according to WCAG, lots of literature is not accessible to 
> everyone who speaks the base language it was written in. 
> Which strikes me as uncontroversial.
> 
> Cheers
> 
> Chaals
> 
> >Lisa:
> >Words written in a different alphabet to that of the primary natural 
> >language of the plain are foreign words and should have a 
> translation 
> >provided...
> >
> >> Lisa, earlier:
> >> > In Hebrew (for once ) this is easy.
> >> > A foreign word is written in a different character set.
> >>Jens:
> >> CMIIW, but since the UCS (Universal Character Set, often  
> referred to 
> >>as
> >> Unicode) is the document character set for HTML/XML, they  
> (foreign 
> >>words) ain't written in a different character set.
> >>
> >> Again referring to to John (see my last post [1]) I claim 
> this is an 
> >> issue where unimpaired users are affected as well. Also, I 
> don't see 
> >> any need for ruling language use by the WAI WG (there already was 
> >> such a discussion a few months ago [2] ;).
> 



