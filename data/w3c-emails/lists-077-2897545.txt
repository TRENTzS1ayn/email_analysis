docno="lists-077-2897545"
received="Wed Feb 14 11:15:57 2001"
isoreceived="20010214161557"
sent="Wed, 14 Feb 2001 10:15:52 -0600"
isosent="20010214161552"
name="Martin McCormick"
email="martin@dc.cis.okstate.edu"
subject="Re: [media] Making Sites Accessible Makes Sense For All Customers"
id="E14T4au-0006h3-00@dc.cis.okstate.edu"
inreplyto="[media] Making Sites Accessible Makes Sense For All Customers"
expires="-1"

To: WAI Interest Group<w3c-wai-ig@w3.org>


"Phill Jenkins" writes:
>When did "free and open-source" become a criteria for
>accessibility?

About 50 years ago.  Well, just kidding a little, but the
concept of backwards compatibility or a base level of
functionality goes way, way back in the history of
telecommunications.  In the very early fifties, the US Federal
Communications Commission said that private industry could duke
it out as to which color TV standard was best, but two conditions
had to be met.

1.  A color signal had to occupy no more band width than a
black-and-white channel.

2.  The color information had to not ruin the picture for an
existing black-and-white television set.  This was precisely to
keep from having to throw out possibly new equipment that
happened to be good, but not color.

Actually, there was a third requirement that stated that
which ever color standard worked best after an engineering trial
period would be the one single standard used by the whole
country.

The same set of criteria applied about ten years later
when FM stereo sound came about and about 20 or so years later
when television adopted the stereophonic sound with SAP
capabilities.

Now, hang on a minute.  I am not saying that this
produced the best technology possible or that computing should be
regulated by the FCC.  I am simply saying that it is in a lot of
people's interest to have information technology that works over
a wide range of clients, from the old P.C.'s in a dusty school
lab in a poor part of town to the cutting-edge systems whose
owners have lots of money and who will call them obsolete next
month.

For UNIX users who also use access technology, the vast
majority of applications work fine for us without spending extra
money or going to extra trouble.

>  JavaScript, and it's international standard ECMAScript,
>was invented to solve some real problems.  Why can't lynx "get with it" and
>support JavaScript?  This reminds me of the days when people were ranting
>and raving about GUIs being in-accessible.

They still are.  What we end up doing is coming up with
engines that basically sledge-hammer the GUI until it looks at
the user end as much like a command line as is practical.  It
also seems interesting that there is no sacrosanct strategy to
make sure that new GUI applications are accessible.

Javascript may have been invented to solve some problems,
but I am not sure which.  I do know it created lots of new problems,
also.

What about the hand-held cell phones with tiny screens
and limited computing resources?  What about the Internet
appliances that are basically browsers frozen in silicon?

I said on another list that there is nothing sacred about
lynx.  A text-based browser that supports javascript will be fine
until next month or the next phase of the Moon when some new or
different technology comes out to use megabytes of band width to
send a single 8.5X11 sheet of ASCII text.

Another good idea might be to design a text-based engine
for mozilla so that it gets carried along as the rest of it
grows.

True accessibility to me means that the information can
be easily received without undue complexity.

This isn't about dumming down the web even a little bit.
This is about smartening up server technology so that it fits the
client.

Today, it is lynx that has the problem.  What will it be
tomorrow?

Just because something new works doesn't mean that
established concepts should be discontinued.

Martin McCormick



