docno="lists-083-5448940"
received="Wed Nov 24 18:24:26 1999"
isoreceived="19991124232426"
sent="Wed, 24 Nov 1999 17:23:56 -0600"
isosent="19991124232356"
name="thatch@us.ibm.com"
email="thatch@us.ibm.com"
subject="Re: Techniques for 2.2.3"
id="85256833.008090B3.00@d54mta08.raleigh.ibm.com"
charset="us-ascii"
inreplyto="Techniques for 2.2.3"
expires="-1"

To: Al Gilman<asgilman@iamdigex.net>
cc:w3c-wai-ua@w3.org




For goodness sake. The assertion is about "many assistive technologies"
recognizing and responding to language tags in documents. Festival is a
speech synthesis project. I, personally, do not consider that to be an
assistive technology. Raman has worked with audio style sheets, but that
doesn't say that he is language switching based on language tags; maybe he
does. Many?

Jim Thatcher


Al Gilman <asgilman@iamdigex.net> on 11/24/99 05:33:50 PM

To:   w3c-wai-ua@w3.org
cc:
Subject:  Re: Techniques for 2.2.3




At 12:12 PM 11/24/99 -0600, thatch@us.ibm.com wrote:
>
>
>> Many assistive technologies understand different languages and can
render
>> them according to the language attribute defined for a certain part of
>the
>> document.
>
>I don't know of any such assistive technologies.

Doesn't the ACSS implementation in EmacSpeak let you do this?

I don't know many others, but the Festival TTS system from Edinburgh does:

 http://www.cstr.ed.ac.uk/projects/festival/

Al

>
>Jim Thatcher
>IBM Special Needs Systems
>www.ibm.com/sns
>HPR Documentation page: http://www.austin.ibm.com/sns/hprdoc.html
>thatch@us.ibm.com
>(512)838-0432
>
>
>Marja-Riitta Koivunen <marja@w3.org> on 11/24/99 11:58:52 AM
>
>To:   w3c-wai-ua@w3.org
>cc:
>Subject:  Techniques for 2.2.3
>
>
>
>
>I hope this is OK as starting point. Gregory promised to make another
>iteration.
>
>Marja
>
>2.3 Render content according to natural language identification
>
>Let user select the default natural language or languages in priority
order
>that she normally prefers to receive content.  As content in the preferred
>language might not always be available, the user needs to be able to see
>what languages are available in the current presentation and select from
>these.
>
>Many assistive technologies understand different languages and can render
>them according to the language attribute defined for a certain part of the
>document. For  instance, a screen reader might change the pronunciation of
>the text according to the language definition. This is usually desired and
>done according to the capabilities of the tool. Some specialized tools
>might give some finer user control for the pronunciation as well.
>
>Sometimes the user might also want to know when the text contains parts in
>other languages. How to render the change of language should be made user
>controllable by the user agent. For instance, the user might choose to
hear
>"language:new language e.g. German" when the language changes to German
and
>"language: default language" when it changes back. Alternatively or in
>addition, the language change could also be rendered visually as text
>withing the document. User should be able to turn this on or off as it
>might be disturbing to users understanding the languages. (Maybe the UA
>could use stylesheets for implementing the change when available.) In
>addition, if possible the UA might have interpretations available behind a
>link or provide a separate function for that.
>



