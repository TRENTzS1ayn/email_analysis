docno="lists-083-5461023"
received="Wed Nov 24 18:30:27 1999"
isoreceived="19991124233027"
sent="Wed, 24 Nov 1999 18:29:21 -0500"
isosent="19991124232921"
name="Marja-Riitta Koivunen"
email="marja@w3.org"
subject="Questions about WCAG 1.3"
id="3.0.5.32.19991124182921.00b4faa0@localhost"
charset="us-ascii"
expires="-1"


To:w3c-wai-gl@w3.org
Cc:w3c-wai-ua@w3.org

I have some questions concerning WCAG CP 1.3 that came up at the UA meeting
and I promised to send to GL group.

Checkpoint:
1.3 Until user agents can automatically read aloud the text equivalent of a
visual track, provide an auditory description of the important information
of the visual track of a multimedia presentation. [Priority 1]  Synchronize
the auditory description with the audio track as per checkpoint 1.4. Refer
to checkpoint 1.1 for information about textual equivalents for visual
information.  Techniques for checkpoint 1.3 

Questions:
I was trying to think how this checkpoint could be implemented
in the user agent. First question is what the author actually provides when
he provides the text equivalent of a visual track? It seems that it is
something that can be used to create auditory description. So it needs to
be a continuous text stream that is synchronized to the video as it is
describing the contents of the video.

Why would someone create such a textstream? A collated text transcript
that can be read independently from the video would make more sense to me.
If a user can see text why not look the video rather than the description?
Are there users that have hard time interpreting the video and that's why?
When the device does not have a screen where to show the video it seems
that collated text makes more sense.

A textstream need to be synchronized so that there is enough time to read
it. The synchronization of the text that is visually read might be
different for text than for the automatically created audio. So to
automatically create an audio description based on text stream that is
synchronized in a right way with audio might be difficult. Especially as
the synchronization might change the timing of the original video as well
if the natural pauses are not long enough to include the audio
descriptions. Is there any ideas how the synchronization is created
automatically from the textstream?

Shortly: What does the author actually provide as text equivalent and how
should the UA or the media player create the audio description from that?

Marja



