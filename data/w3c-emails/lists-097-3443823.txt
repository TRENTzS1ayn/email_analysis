docno="lists-097-3443823"
received="Thu Mar 21 02:20:32 2002"
isoreceived="20020321072032"
sent="Mon, 18 Mar 2002 14:55:06 -0500 (EST)"
isosent="20020318195506"
name="Laurent Denoue"
email="Denoue@fxpal.com"
subject="RE: Orphaned annotations"
id="B424F08112D9F54087F29B0066FD00001A44C3@pobox.fxpal.com"
charset="iso-8859-1"
inreplyto="Orphaned annotations"
expires="-1"


To:"David M Bargeron"<davemb@microsoft.com>,"Jim Ley"<jim@jibbering.com>,<www-annotation@w3.org>

David (and Jim further down this email)

1- Yes to "human-level" content
I deeply agree with you about anchoring to "human-level" content.
In Yawas, I also use the same approach where annotations
are anchored to the annotated content (namely the highlighted text).
But I also found it useful to store the occurrence
of the content you annotate in the document (e.g. you
annotate the second occurrence of "human-level" in your email: Yawas
stores (2,"human-level").

As you pointed out, the nice effect of anchoring to the content
is that you can map annotations from one document format to another.
In Yawas, I found it occurring on the web itself with the same web page
hosted on different servers.

For simplicity, Yawas currently anchors annotations to one URL.
But I will certainly now have to compute a kind of content-based
document signature instead of URL: this will allow Yawas to map
annotations correctly, independently of the URL. Maybe later I will implement
mapping to different document formats (e.g. sometimes it's true that I annotate
a paper in HTML format with Yawas and later on find the PDF or DOC version: it would
be neat to map my annotations).

By the way, congratulations for your excellent work on repositioning annotations
(both the TR and paper which I've read before).

2- Some issues with image annotation
Regarding annotating images that Jim evoked, we are also working on XLibris
here at FXPAL (Yawas was done before that when I was doing my PhD in France).
XLibris supports freeform annotations over text and/or images.
An annotation can overlap an image. If the annotation overlaps
more the image than the text, then we attach the annotation to this image.

As for the textual content of a document, image content can also change and their annotations
need to be repositioned. As for text, it would certainly make sense to use a
"human-level" content. Previous work on this area includes annotation of video,
where annotations can be mapped sometimes in real-time using image tracking techniques.

3- Peer-to-peer annotations
I'm also very interested in USING all these annotations.
I think annotation servers are a good way to start.
Storing all annotations on an annotation server is like forcing you to send your bookmarks to a bookmarks server.
I would rather see a more peer-to-peer solution where standalone annotations can be embedded in documents,
not requiring any annotation server. Like people do when they take their interesting bookmarks and author a web page.
As with hyperlinks, everyone could easily author annotations.
An example is http://www.cnn.com#annotation-content+text=+from=laurent
(Tomas Phelps and Robert Wilensky used a similar encoding for their robust urls).

Possibly, existing search engines could crawl the web and index these embedded annotations.
When a user accesses a document, he/she could ask for the annotations on this content (regardless of the URL,
document format). For more private annotations (e.g. a group of people working together around a document), you could also point your annotation client to a set of web pages likely to contain annotations relevant to this group as you do it today in Annotea clients by pointing them to an Annotea server.

I'd like to get some feedback about this idea of peer-to-peer annotations.
Laurent.



