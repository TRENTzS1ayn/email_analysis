docno="lists-098-10947431"
received="Sun Mar 31 18:13:56 2002"
isoreceived="20020331231356"
sent="Sun, 31 Mar 2002 18:13:55 -0500 (EST)"
isosent="20020331231355"
name="Charles McCathieNevile"
email="charles@w3.org"
subject="Re: visual and auditory navigation: examples needed"
id="Pine.LNX.4.30.0203311812070.12271-100000@tux.w3.org"
charset="US-ASCII"
inreplyto="Version.32.20020331094011.01b66e50&#64;pop.iamdigex.net"
expires="-1"

To: Al Gilman<asgilman@iamdigex.net>
cc: jonathan chetwynd<j.chetwynd@btinternet.com>,<www-archive@w3.org>


On Sun, 31 Mar 2002, Al Gilman wrote:

  - Dave Bolnik did a multimedia example that did word highlighting in sync
  to the speech that he showed at CSUN a couple years back.  This however
  is a canned presentation mechanically following a programmed timeline.
  Not an on-the-fly derived timeline based on interaction with a user.
  This was done with SAMI.  If I understand the concept of the SMART
  technology proposition, it is to create an industry-consensus SAMI
  workalike, in rough terms.

websound, braillesurf both do something along these lines already, working
automatically. I think EIAD might too.



