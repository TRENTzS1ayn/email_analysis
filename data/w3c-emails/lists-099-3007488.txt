docno="lists-099-3007488"
received="Fri Jan  3 18:36:59 2003"
isoreceived="20030103233659"
sent="03 Jan 2003 18:36:14 -0500"
isosent="20030103233614"
name="Daniel \"
email="eikeon@eikeon.com"
subject="Re: w3.org archival of #rdfig logs and chump"
id="1041636980.21987.62.camel@panther"
inreplyto="20030103231818.GA21780&#64;tux.w3.org"
expires="-1"

To: Dan Brickley<danbri@w3.org>
Cc: Edd Dumbill<edd@usefulinc.com>, Gerald Oskoboiny<gerald@w3.org>, Dave Beckett<dave.beckett@bristol.ac.uk>,connolly@w3.org,www-archive@w3.org



On Fri, 2003-01-03 at 18:18, Dan Brickley wrote:
> 
> * Edd Dumbill <edd@usefulinc.com> [2003-01-03 20:35+0000]
> > I'm fine with a crawler, you're right about the bandwidth requirements
> > -- especially as each day there are at most 4 files which change in the
> > chump hierarchy (front page, archived day, month, year.)
> > 
> > A simple Perl script could mirror the chump stuff quite happily.
> 
> Yep, Gerald's right. I guess I figured the laziest, simplest thing 
> was just grabbing the single tar.gz, but probably best to get the 4 pages 
> instead.

Just in case I though I would toss this out there... Is the data for the
4 pages available as RDF? If so, I would be willing to contribute a
script that would crawl the pages and keep an InformationStore (or
TripleStore) up-to-date so that the RDF is archived. And could also
contribute code for generating an HTML view of the RDF etc.

-- 
Daniel Krech, http://eikeon.com/

Redfoot.net, http://redfoot.net/
RDFLib.net, http://rdflib.net/



