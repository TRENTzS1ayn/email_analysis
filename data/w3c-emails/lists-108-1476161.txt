docno="lists-108-1476161"
received="Wed Feb 25 21:25:35 1998"
isoreceived="19980226022535"
sent="Thu, 26 Feb 98 02:22:02 +0000"
isosent="19980226022202"
name="Clive Bruton"
email="clive@typonaut.demon.co.uk"
subject="RE: Origin of Verdana"
id="1323662609-1686535@battersea.indx.co.uk"
charset="US-ASCII"
inreplyto="Origin of Verdana"
expires="-1"

To:<www-font@w3.org>


Bill Hill wrote at 26/02/98 1:10 am

>The Macintosh's nominal resolution has remained unchanged since it was
>launched in 1984. There were good reasons at that time for picking 72dpi,
>since 1 pixel = 1 point. If you were concerned about print publishing - and
>of course the Mac was very focused on that from early on - then this
>resolution made good sense.
>
>I remember when I worked for Aldus in Europe (Nov 86-Oct94), and I saw the
>first version of PageMaker running on a PC. With a Hercules graphics card
>(non-square pixels) it looked horrible. I was appalled when I compared it to
>my beloved Macintosh.
>
>Since then, though, PC baseline resolution has improved by leaps and bounds.
>96dpi is pretty much the "lowest common denominator" resolution we have
>today on Windows machines. Don't ask me "Why 96?" - but it's better than 72.

As you already pointed out 72dpi is nominal on MacOS, it isn't mandatory 
or absolute. Most shipping Macintosh models run up to 1200*1600 displays, 
you'll find this is around 120dpi on an AppleVision 850 monitor.

I've a very ancient Quadra 950, it runs a monitor at about 100dpi.


>I find it really strange, when the Macintosh has such a share of the
>graphics and multimedia markets, that its baseline resolution has never
>changed in 14 years. What else in the computer industry is the same as it
>was 14 years ago? Laser printer resolution went from 300dpi to 600dpi and
>now 1200dpi in the same period.

Baseline resolution is in reality the number of pixels you have to play 
with, not the size of those pixels, ie you can resolve something to a 
higher degree the more pixels you have. 1200*1600 seems pretty high-res 
to me.

>
>The more pixels you have, the better job you can do when displaying type on
>the screen.

Exactly.


>As you go down in resolution, and also drop the size of your
>type, you get to a certain point where there just aren't enough pixels to do
>the job.

ie a small number of pixels, not a low dpi.

>But until the Macintosh
>resolution changes, small type is always going to look better on Windows
>than the Macintosh at the same size because of the scaling problem you
>highlight.

Small type, in pixels per em, isn't going to look any different on a Mac 
than it does on anything else, unless someone screwed up real bad.

>
>I guess the only solution today is that site designers need to take into
>account how their site will display in a cross-platform environment and
>specify type accordingly. You get less on the screen, but at least it's
>readable. A lot of designers who put their sites together on Windows
>probably aren't even aware there's a problem!

There's only a problem if one decides to spec type in a resolution 
dependent way, ie in pixels, and that problem would most probably be 
worse for "nominal" Windows users than "nominal" Mac users, since at 
96dpi those pixels will actually be smaller.

The W3 recommendation for specifying type sizes for style is to do so in 
a resolution independent manner, to do otherwise would be more than a 
little stupid, given the range of displays any particular implementation 
is likely to encounter.

>
>As a user, of course you have the option of increasing the default font size
>in the browser. Doesn't help, though, if it breaks the carefully-planned
>design.

That sounds like carefully planned design, poorly implemented execution.


-- Clive



